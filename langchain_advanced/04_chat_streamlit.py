# ëª©ì : ë¸Œë¼ìš°ì €ì—ì„œ ëŒ€í™”í•˜ëŠ” ê°„ë‹¨ UI
# í¬ì¸íŠ¸:
#  - streamlitì˜ st.chat_input, st.chat_messageë¡œ ì±— í˜•íƒœ UI êµ¬ì„±
#  - ì„¸ì…˜ ìƒíƒœ(st.session_state)ì— (role, content) íŠœí”Œì„ ìŒ“ì•„ ê°„ë‹¨íˆ íˆìŠ¤í† ë¦¬ ìœ ì§€
#  - ë§¤ìš° ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ í”„ë¡¬í”„íŠ¸ì— íˆìŠ¤í† ë¦¬ë¥¼ "ë¬¸ìì—´"ë¡œ ì´ì–´ë¶™ì—¬ ëª¨ë¸ì— ì „ë‹¬
#    (ì‹¤ì „ì—ì„œëŠ” LangChain ë©”ëª¨ë¦¬(RunnableWithMessageHistory)ë¥¼ ì„œë²„ ì‚¬ì´ë“œë¡œ ë„£ëŠ” í¸ì´ ë” ì•ˆì „)

import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

# í˜ì´ì§€ ë©”íƒ€ ì„¤ì •(ë¸Œë¼ìš°ì € íƒ­ ì œëª©/ì•„ì´ì½˜)
st.set_page_config(page_title="LangChain Chat", page_icon="ğŸ’¬")

# ==========================
# [ADDED] ì‚¬ì´ë“œë°”: ëª¨ë¸/ì˜¨ë„ ì„¤ì •
# ==========================
with st.sidebar:
    st.subheader("ì„¤ì •")
    model = st.selectbox("ëª¨ë¸", ["gpt-4o-mini", "gpt-4o"], index=0)
    temp = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
    st.caption("ëª¨ë¸ê³¼ ìƒ˜í”Œë§ ê°•ë„(Temperature) ì„¤ì •")

# ==========================
# [MODIFIED] ì‚¬ì´ë“œë°” ì„ íƒê°’ìœ¼ë¡œ LLM ìƒì„±
# ==========================
llm = ChatOpenAI(model=model, temperature=temp)

# ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    # ("user"|"assistant", content)
    st.session_state["messages"] = []

st.title("LangChain ê³¼ OpenAI ë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ ì±—ë´‡ êµ¬í˜„ğŸ¤– ")

# ê³¼ê±° ë©”ì‹œì§€ë¥¼ UIì— ë Œë”
for role, content in st.session_state["messages"]:
    with st.chat_message(role):
        st.markdown(content)

# í•˜ë‹¨ ì…ë ¥ì°½ (ì—”í„°ë¡œ ì „ì†¡)
user = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
if user:
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€/í‘œì‹œ
    st.session_state["messages"].append(("user", user))
    with st.chat_message("user"):
        st.markdown(user)

    # 2) ê°„ë‹¨í•œ íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ì„ ë§Œë“¤ì–´ ëª¨ë¸ì— ì „ë‹¬
    #    (ìµœê·¼ 6í„´ë§Œ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê³¼ë„ ì¦ê°€ë¥¼ ë°©ì§€)
    history_text = "\n".join([f"{r.upper()}: {c}" for r, c in st.session_state["messages"][-6:]])
    prompt = (
        "ë‹¤ìŒ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì— ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼.\n"
        f"{history_text}\n\në‹µë³€:"
    )

    # 3) ëª¨ë¸ í˜¸ì¶œ
    answer = llm.invoke(prompt).content

    # 4) ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€/í‘œì‹œ
    st.session_state["messages"].append(("assistant", answer))
    with st.chat_message("assistant"):
        st.markdown(answer)
